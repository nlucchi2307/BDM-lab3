{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "27abb3ed",
   "metadata": {},
   "source": [
    "# Task B"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5deb1b6a",
   "metadata": {},
   "source": [
    "The steps performed will be:\n",
    "\n",
    "- Model Training\n",
    "\n",
    "- Model Management\n",
    "\n",
    "    Use MLflow (or a similar model management framework) to track the entire pipeline, including models, hyperparameters, evaluation metrics, and tagging the best model for deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "174d8838",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: c:\\BDM3\n",
      "MLflow tracking URI: file:///C:/BDM3/mlruns\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "import os\n",
    "\n",
    "# Check and print your current working directory\n",
    "print(\"Current working directory:\", os.getcwd())\n",
    "\n",
    "# Set MLflow tracking URI to a local path OUTSIDE OneDrive\n",
    "mlflow.set_tracking_uri(\"file:///C:/BDM3/mlruns\")\n",
    "print(\"MLflow tracking URI:\", mlflow.get_tracking_uri())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "93900788",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, avg, sqrt, abs as spark_abs\n",
    "from pyspark.ml.regression import LinearRegression, RandomForestRegressor, GBTRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7a187df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set a Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"PredictiveAnalysis\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4e1bd055",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the path for exploitation zone\n",
    "exploitation_zone = \"exploitation_zone\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "55b367be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load preprocessed data\n",
    "train_data = spark.read.parquet(f\"{exploitation_zone}/train_data\")\n",
    "test_data = spark.read.parquet(f\"{exploitation_zone}/test_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "af7a7e6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+--------------------+\n",
      "|price_clean|            features|        neighborhood|\n",
      "+-----------+--------------------+--------------------+\n",
      "|    39000.0|(91,[0,1,2,3,4,5,...|El Poble Sec - Pa...|\n",
      "|    60000.0|(91,[0,1,2,3,4,5,...|          La Bordeta|\n",
      "|    69500.0|(91,[0,1,2,3,4,5,...|               Sants|\n",
      "|    70000.0|(91,[0,1,2,3,4,5,...|  La Marina del Port|\n",
      "|    79900.0|(91,[0,2,3,4,5,6,...|El Poble Sec - Pa...|\n",
      "+-----------+--------------------+--------------------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "train_data.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d15990f",
   "metadata": {},
   "source": [
    "## Model Training and Management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "562e4246",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we create 3 different ML models: Linear Regression, Random Forest, and Gradient Boosted Trees\n",
    "\n",
    "# Linear Regression model\n",
    "linear_regression = LinearRegression(\n",
    "    featuresCol=\"features\", \n",
    "    labelCol=\"price_clean\",\n",
    "    predictionCol=\"prediction\")\n",
    "\n",
    "# Random Forest model\n",
    "random_forest = RandomForestRegressor(\n",
    "    featuresCol=\"features\",\n",
    "    labelCol=\"price_clean\", \n",
    "    predictionCol=\"prediction\",\n",
    "    numTrees=50,\n",
    "    maxDepth=10,\n",
    "    seed=42)\n",
    "\n",
    "# Gradient Boosted Trees model\n",
    "gradient_boosting = GBTRegressor(\n",
    "    featuresCol=\"features\",\n",
    "    labelCol=\"price_clean\",\n",
    "    predictionCol=\"prediction\", \n",
    "    maxIter=50,\n",
    "    maxDepth=8,\n",
    "    seed=42)\n",
    "\n",
    "models = {\n",
    "    \"Linear Regression\": linear_regression,\n",
    "    \"Random Forest\": random_forest, \n",
    "    \"Gradient Boosting\": gradient_boosting}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1e0cd68e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define parameter grids for each model to optimize hyperparameters\n",
    "\n",
    "# Linear Regression parameter grid\n",
    "lr_param_grid = ParamGridBuilder() \\\n",
    "    .addGrid(linear_regression.regParam, [0.01, 0.1]) \\\n",
    "    .addGrid(linear_regression.elasticNetParam, [0.0, 0.5]) \\\n",
    "    .build()\n",
    "\n",
    "# Random Forest parameter grid\n",
    "rf_param_grid = ParamGridBuilder() \\\n",
    "    .addGrid(random_forest.numTrees, [30, 50]) \\\n",
    "    .addGrid(random_forest.maxDepth, [5, 10]) \\\n",
    "    .build()\n",
    "\n",
    "# Gradient Boosting parameter grid  \n",
    "gb_param_grid = ParamGridBuilder() \\\n",
    "    .addGrid(gradient_boosting.maxIter, [30, 50]) \\\n",
    "    .addGrid(gradient_boosting.maxDepth, [5, 8]) \\\n",
    "    .build()\n",
    "\n",
    "param_grids = {\n",
    "    \"Linear Regression\": lr_param_grid,\n",
    "    \"Random Forest\": rf_param_grid,\n",
    "    \"Gradient Boosting\": gb_param_grid}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd002fbb",
   "metadata": {},
   "source": [
    "## MLflow tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7539300e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/06/24 16:51:12 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression | CV RMSE: 198031.99 | Test RMSE: 182560.91 | Test MAE: 115873.27 | Test R²: 0.791\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/06/24 16:52:02 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest | CV RMSE: 177628.45 | Test RMSE: 153586.39 | Test MAE: 91361.98 | Test R²: 0.852\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/06/24 16:56:49 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Boosting | CV RMSE: 198784.82 | Test RMSE: 159640.09 | Test MAE: 98805.91 | Test R²: 0.840\n"
     ]
    }
   ],
   "source": [
    "# Define evaluators\n",
    "evaluator_rmse = RegressionEvaluator(labelCol=\"price_clean\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "evaluator_mae  = RegressionEvaluator(labelCol=\"price_clean\", predictionCol=\"prediction\", metricName=\"mae\")\n",
    "evaluator_r2   = RegressionEvaluator(labelCol=\"price_clean\", predictionCol=\"prediction\", metricName=\"r2\")\n",
    "\n",
    "best_models = {}\n",
    "\n",
    "for model_name, param_grid in param_grids.items():\n",
    "    with mlflow.start_run(run_name=model_name):\n",
    "        cv = CrossValidator(\n",
    "            estimator=models[model_name],\n",
    "            estimatorParamMaps=param_grid,\n",
    "            evaluator=evaluator_rmse,\n",
    "            numFolds=2\n",
    "        )\n",
    "        cv_model = cv.fit(train_data)\n",
    "        best_model = cv_model.bestModel\n",
    "\n",
    "        # CV RMSE for the best param set\n",
    "        best_cv_rmse = min(cv_model.avgMetrics)\n",
    "\n",
    "        # Predict on validation/test set\n",
    "        predictions = best_model.transform(test_data)\n",
    "        test_rmse = evaluator_rmse.evaluate(predictions)\n",
    "        test_mae  = evaluator_mae.evaluate(predictions)\n",
    "        test_r2   = evaluator_r2.evaluate(predictions)\n",
    "\n",
    "        # Log hyperparameters (from bestModel's params)\n",
    "        for param, value in best_model.extractParamMap().items():\n",
    "            mlflow.log_param(str(param), value)\n",
    "\n",
    "        # Log evaluation metrics\n",
    "        mlflow.log_metric(\"cv_rmse\", best_cv_rmse)\n",
    "        mlflow.log_metric(\"test_rmse\", test_rmse)\n",
    "        mlflow.log_metric(\"test_mae\", test_mae)\n",
    "        mlflow.log_metric(\"test_r2\", test_r2)\n",
    "\n",
    "        # Log model\n",
    "        mlflow.spark.log_model(best_model, \"model\")\n",
    "\n",
    "        # Store everything for later comparison\n",
    "        best_models[model_name] = {\n",
    "            \"model\": best_model,\n",
    "            \"cv_rmse\": best_cv_rmse,\n",
    "            \"test_rmse\": test_rmse,\n",
    "            \"test_mae\": test_mae,\n",
    "            \"test_r2\": test_r2\n",
    "        }\n",
    "        print(f\"{model_name} | CV RMSE: {best_cv_rmse:.2f} | Test RMSE: {test_rmse:.2f} | Test MAE: {test_mae:.2f} | Test R²: {test_r2:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aa6754e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "                    MODEL PERFORMANCE COMPARISON\n",
      "================================================================================\n",
      "Model                CV RMSE      Test RMSE    Test MAE     Test R²   \n",
      "--------------------------------------------------------------------------------\n",
      "Linear Regression    €198,032     €182,561     €115,873     0.791     \n",
      "Random Forest        €177,628     €153,586     €91,362      0.852     \n",
      "Gradient Boosting    €198,785     €159,640     €98,806      0.840     \n",
      "--------------------------------------------------------------------------------\n",
      "================================================================================\n",
      "\n",
      "Best model by test RMSE: Random Forest (RMSE = €153,586)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"                    MODEL PERFORMANCE COMPARISON\")\n",
    "print(\"=\"*80)\n",
    "print(f\"{'Model':<20} {'CV RMSE':<12} {'Test RMSE':<12} {'Test MAE':<12} {'Test R²':<10}\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "best_model_name = None\n",
    "best_rmse = float('inf')\n",
    "\n",
    "for model_name, info in best_models.items():\n",
    "    print(f\"{model_name:<20} €{info['cv_rmse']:<11,.0f} €{info['test_rmse']:<11,.0f} €{info['test_mae']:<11,.0f} {info['test_r2']:<10.3f}\")\n",
    "    if info['test_rmse'] < best_rmse:\n",
    "        best_rmse = info['test_rmse']\n",
    "        best_model_name = model_name\n",
    "\n",
    "print(\"-\"*80)\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nBest model by test RMSE: {best_model_name} (RMSE = €{best_rmse:,.0f})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "43542276",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/06/24 16:57:20 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n"
     ]
    }
   ],
   "source": [
    "#  Storing the best model\n",
    "best_model = best_models[best_model_name][\"model\"]\n",
    "\n",
    "with mlflow.start_run(run_name=\"best_model_final\"):\n",
    "    # Log all hyperparameters\n",
    "    for param, value in best_model.extractParamMap().items():\n",
    "        mlflow.log_param(str(param), value)\n",
    "    \n",
    "    # Log evaluation metric(s)\n",
    "    mlflow.log_metric(\"rmse\", best_models[best_model_name][\"test_rmse\"])\n",
    "    \n",
    "    # Log the model itself\n",
    "    mlflow.spark.log_model(best_model, \"model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b9a8eb42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save with Spark Also\n",
    "best_model.write().overwrite().save(f\"{exploitation_zone}/best_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7afd666b",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
